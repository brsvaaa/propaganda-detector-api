'''
# server.py
import os
os.environ["CUDA_VISIBLE_DEVICES"] = ""
import logging
import urllib.request

from flask import Flask, request, jsonify
from flask_cors import CORS, cross_origin

import spacy
import numpy as np
import joblib

import torch
import torch.nn.functional as F
from transformers import XLNetTokenizer, XLNetForSequenceClassification

import keras
from keras.layers import InputLayer
from keras.models import load_model

from huggingface_hub import hf_hub_download


# ========== –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ==========
MODEL_DIR = "models"
os.makedirs(MODEL_DIR, exist_ok=True)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
logging.basicConfig(level=logging.INFO)

app = Flask(__name__)
CORS(app)

# ========== –ü–∞—Ç—á –¥–ª—è InputLayer ==========
class CustomInputLayer(InputLayer):
    def __init__(self, **kwargs):
        batch_shape = kwargs.pop('batch_shape', None)
        if batch_shape is not None:
            # –£–±–∏—Ä–∞–µ–º batch_shape –∏ —Å—Ç–∞–≤–∏–º input_shape = (dims...,)
            kwargs['input_shape'] = tuple(batch_shape[1:])
        super().__init__(**kwargs)

MODELS = None

# ========== –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π ==========
def init_models():
    logging.info("‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π‚Ä¶")

    # 1) –°–∫–∞—á–∏–≤–∞–µ–º –≤—Å–µ `.keras` –∏ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã –∏–∑ HF
    hf_repos = {
        "Appeal_to_Authority_model.keras": "brsvaaa/Appeal_to_Authority_model.keras",
        "Bandwagon_Reductio_ad_hitlerum_model.keras": "brsvaaa/Bandwagon_Reductio_ad_hitlerum_model.keras",
        "Black-and-White_Fallacy_model.keras": "brsvaaa/Black-and-White_Fallacy_model.keras",
        "Causal_Oversimplification_model.keras": "brsvaaa/Causal_Oversimplification_model.keras",
        "Slogans_model.keras": "brsvaaa/Slogans_model.keras",
        "Thought-terminating_Cliches_model.keras": "brsvaaa/Thought-terminating_Cliches_model.keras",
        "text_classification_model.keras": "brsvaaa/text_classification_model.keras",
        "vectorizer.joblib": "brsvaaa/vectorizer.joblib",
        "label_encoder.joblib": "brsvaaa/label_encoder.joblib"
    }
    local = {}
    for fname, repo in hf_repos.items():
        path = hf_hub_download(repo_id=repo, filename=fname, cache_dir=MODEL_DIR, repo_type="model")
        local[fname] = path
        logging.info(f"‚úÖ {fname} —Å–∫–∞—á–∞–Ω –≤ {path}")

    models = {}
    # 2) TF-IDF + LabelEncoder
    models['tfidf'] = joblib.load(local["vectorizer.joblib"])
    models['le']    = joblib.load(local["label_encoder.joblib"])

    # 3) Keras-–º–æ–¥–µ–ª–∏
    def load_keras(key):
        return load_model(
            local[key],
            custom_objects={
                'Functional': keras.models.Model,
                'InputLayer': CustomInputLayer
            },
            compile=False
        )

    models['mc_keras']    = load_keras("text_classification_model.keras")
    models['bin_auth']    = load_keras("Appeal_to_Authority_model.keras")
    models['bin_band']    = load_keras("Bandwagon_Reductio_ad_hitlerum_model.keras")
    models['bin_bwfall']  = load_keras("Black-and-White_Fallacy_model.keras")
    models['bin_causal']  = load_keras("Causal_Oversimplification_model.keras")
    models['bin_slog']    = load_keras("Slogans_model.keras")
    models['bin_thou']    = load_keras("Thought-terminating_Cliches_model.keras")

    # 4) XLNet —á–µ—Ä–µ–∑ PyTorch
    models['xlnet_tok'] = XLNetTokenizer.from_pretrained("xlnet-base-cased")
    models['xlnet_mc']  = XLNetForSequenceClassification.from_pretrained("brsvaaa/xlnet_trained_model")

    # 5) spaCy
    models['nlp'] = spacy.load("en_core_web_sm")

    logging.info("‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ.")
    return models

def get_models():
    global MODELS
    if MODELS is None:
        MODELS = init_models()
    return MODELS

# ========== –£—Ç–∏–ª–∏—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è ==========
def split_sentences(text: str):
    nlp = get_models()['nlp']
    doc = nlp(text)
    return [sent.text.strip() for sent in doc.sents]

def predict_xlnet(text: str):
    m = get_models()
    tok = m['xlnet_tok'](text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    tok = {k: v.to(m['xlnet_mc'].device) for k, v in tok.items()}
    with torch.no_grad():
        logits = m['xlnet_mc'](**tok).logits
    return F.softmax(logits, dim=1).cpu().numpy()

def pad_to_expected(x: np.ndarray, target_dim: int):
    current = x.shape[1]
    if current < target_dim:
        return np.hstack([x, np.zeros((x.shape[0], target_dim-current))])
    return x[:, :target_dim]
    
def predict_keras_mc(text: str):
    m = get_models()
    vec = m['tfidf'].transform([text]).toarray()
    expected = m['mc_keras'].input_shape[-1]
    vec = pad_to_expected(vec, expected)
    return m['mc_keras'].predict(vec)

def predict_binary_label(text: str):
    m = get_models()
    # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º ¬´—Å—ã—Ä–æ–π¬ª –≤–µ–∫—Ç–æ—Ä TF-IDF
    raw_vec = m['tfidf'].transform([text]).toarray()

    # –ü–µ—Ä–µ—á–µ–Ω—å –±–∏–Ω–∞—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π + –∏—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
    binary_models = [
        ('bin_auth',   0),   # Appeal to Authority
        ('bin_band',   2),   # Bandwagon / Reductio ad Hitlerum
        ('bin_bwfall', 3),   # Black-and-White Fallacy
        ('bin_causal', 4),   # Causal Oversimplification
        ('bin_slog',  12),   # Slogans
        ('bin_thou',  13),   # Thought-terminating Cliches
    ]

    for model_key, label_idx in binary_models:
        model = m[model_key]
        # –í—ã—è—Å–Ω—è–µ–º, –∫–∞–∫–æ–π —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è —É —ç—Ç–æ–π –±–∏–Ω–∞—Ä–∫–∏
        expected = model.input_shape[-1]
        # –ü–æ–¥–≥–æ–Ω—è–µ–º –Ω–∞—à raw_vec –ø–æ–¥ —ç—Ç—É –¥–ª–∏–Ω—É
        if raw_vec.shape[1] < expected:
            vec = np.hstack([raw_vec, np.zeros((raw_vec.shape[0], expected - raw_vec.shape[1]))])
        else:
            vec = raw_vec[:, :expected]

        # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        prob = model.predict(vec)
        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º –≤—ã—Ö–æ–¥ (–¥–≤–∞ –Ω–µ–π—Ä–æ–Ω–∞? –∏–ª–∏ –æ–¥–∏–Ω?)
        if prob.ndim == 2 and prob.shape[1] == 2:
            score = float(prob[0, 1])
        else:
            score = float(prob[0, 0])
        if score > 0.5:
            return label_idx

    return None
    
def ensemble_multiclass_predict(text: str):
    
    p1 = predict_xlnet(text)
    p2 = predict_keras_mc(text)
    avg = (p1 + p2) / 2
    cls = int(np.argmax(avg, axis=1)[0])
    return cls, avg


# ========== Flask-—ç–Ω–¥–ø–æ–∏–Ω—Ç—ã ==========
@app.route('/', methods=['GET'])
def index():
    # For a simple JSON response:
    return jsonify({
        "service": "Propaganda Detector API",
        "version": "1.0",
        "endpoints": {
            "health": "/health",
            "predict": "/predict (POST)"
        }
    }), 200

@app.route('/health', methods=['GET'])
def health():
    return jsonify(status="ok"), 200

@app.route('/predict', methods=['POST','OPTIONS'])
@cross_origin(origins='*')
def predict():
    if request.method == 'OPTIONS':
        return jsonify(message='OK'), 200

    data = request.get_json(silent=True) or {}
    text = data.get("text", "").strip()
    if not text:
        return jsonify(error="No text provided"), 400

    sentences = split_sentences(text)
    results = []
    for s in sentences:
        cls, _ = ensemble_multiclass_predict(s)
        results.append({
            "sentence": s,
            "Multiclass_Prediction": cls
        })

    return jsonify(results=results), 200

if __name__ == '__main__':
    port = int(os.getenv("PORT", 5000))
    logging.info(f"–ó–∞–ø—É—Å–∫ –Ω–∞ –ø–æ—Ä—Ç—É {port}")
    app.run(host='0.0.0.0', port=port)


import os
os.environ["CUDA_VISIBLE_DEVICES"] = ""
import logging
import urllib.request
from flask import Flask, request, jsonify
from flask_cors import CORS, cross_origin
import spacy
import numpy as np
import joblib
import torch
import torch.nn.functional as F
from transformers import XLNetTokenizer, XLNetForSequenceClassification
import tensorflow as tf  
import keras
from keras.layers import InputLayer
from keras.models import load_model
from huggingface_hub import hf_hub_download

MODEL_DIR = "models"
os.makedirs(MODEL_DIR, exist_ok=True)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
logging.basicConfig(level=logging.INFO)

app = Flask(__name__)
CORS(app)

MODELS = None

class CustomInputLayer(InputLayer):
    def __init__(self, **kwargs):
        batch_shape = kwargs.pop('batch_shape', None)
        if batch_shape is not None:
            # –£–±–∏—Ä–∞–µ–º batch_shape –∏ —Å—Ç–∞–≤–∏–º input_shape = (dims...,)
            kwargs['input_shape'] = tuple(batch_shape[1:])
        super().__init__(**kwargs)

logging.info("‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π‚Ä¶")

# 1) –°–∫–∞—á–∏–≤–∞–µ–º –≤—Å–µ `.keras` –∏ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã –∏–∑ HF
hf_repos = {
    "Appeal_to_Authority_model.keras": "brsvaaa/Appeal_to_Authority_model.keras",
    "Bandwagon_Reductio_ad_hitlerum_model.keras": "brsvaaa/Bandwagon_Reductio_ad_hitlerum_model.keras",
    "Black-and-White_Fallacy_model.keras": "brsvaaa/Black-and-White_Fallacy_model.keras",
    "Causal_Oversimplification_model.keras": "brsvaaa/Causal_Oversimplification_model.keras",
    "Slogans_model.keras": "brsvaaa/Slogans_model.keras",
    "Thought-terminating_Cliches_model.keras": "brsvaaa/Thought-terminating_Cliches_model.keras",
    "text_classification_model.keras": "brsvaaa/text_classification_model.keras",
    "vectorizer.joblib": "brsvaaa/vectorizer.joblib",
    "label_encoder.joblib": "brsvaaa/label_encoder.joblib"
}
local = {}
for fname, repo in hf_repos.items():
    path = hf_hub_download(repo_id=repo, filename=fname, cache_dir=MODEL_DIR, repo_type="model")
    local[fname] = path
    logging.info(f"‚úÖ {fname} —Å–∫–∞—á–∞–Ω –≤ {path}")

models = {}
binary_models = {}
# 2) TF-IDF + LabelEncoder
models['vectorizer'] = joblib.load(local["vectorizer.joblib"])
models['label_encoder']  = joblib.load(local["label_encoder.joblib"])
label_encoder = models['label_encoder']
class_labels  = list(label_encoder.classes_)


# 3) Keras-–º–æ–¥–µ–ª–∏
def load_keras(key):
    return load_model(
        local[key],
        custom_objects={
            'Functional': keras.models.Model,
            'InputLayer': CustomInputLayer
        },
        compile=False
    )

models['mc_keras']    = load_keras("text_classification_model.keras")
binary_models = [
    load_keras("Appeal_to_Authority_model.keras"),
    load_keras("Bandwagon_Reductio_ad_hitlerum_model.keras"),
    load_keras("Black-and-White_Fallacy_model.keras"),
    load_keras("Causal_Oversimplification_model.keras"),
    load_keras("Slogans_model.keras"),
    load_keras("Thought-terminating_Cliches_model.keras"),
]
binary_model_names = [
    "Appeal_to_Authority",
    "Bandwagon_Reductio_ad_hitlerum",
    "Black-and-White_Fallacy",
    "Causal_Oversimplification",
    "Slogans",
    "Thought-terminating_Cliches",
]
# 4) XLNet —á–µ—Ä–µ–∑ PyTorch
models['xlnet_tok'] = XLNetTokenizer.from_pretrained("xlnet-base-cased")
models['xlnet_mc']  = XLNetForSequenceClassification.from_pretrained("brsvaaa/xlnet_trained_model")
models['xlnet_mc'].eval()  # >>> ADDED: —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –¥–ª—è PyTorch –º–æ–¥–µ–ª–∏

xlnet_tokenizer = models['xlnet_tok']
xlnet_model     = models['xlnet_mc']
# 5) spaCy
models['nlp'] = spacy.load("en_core_web_sm")

logging.info("‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ.")



# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ—Ä–æ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –±–∏–Ω–∞—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
CONF_THRESHOLD = 0.8

# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–¥–æ–ª–∂–Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π)
MAX_SEQ_LEN = 512  # –Ω–∞–ø—Ä–∏–º–µ—Ä, 128 —Ç–æ–∫–µ–Ω–æ–≤; —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

def pad_to_expected(x: np.ndarray, target_dim: int):
    current = x.shape[1]
    if current < target_dim:
        return np.hstack([x, np.zeros((x.shape[0], target_dim-current))])
    return x[:, :target_dim]
# >>> ADDED: –ö–æ–º–ø–∏–ª—è—Ü–∏—è (–ø—Ä–æ–≥—Ä–µ–≤) –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ö–æ–ª–æ—Å—Ç–æ–º –ø—Ä–∏–º–µ—Ä–µ, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å retracing:contentReference[oaicite:5]{index=5}
# –°–æ–∑–¥–∞—ë–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–π –±–∞—Ç—á –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≥—Ä–µ–≤–∞ Keras –º–æ–¥–µ–ª–µ–π
dummy_tfidf = models['vectorizer'].transform([""]).toarray()
mc_expected = models['mc_keras'].input_shape[-1]
mc_dummy = pad_to_expected(dummy_tfidf, mc_expected).astype(np.float32)
# –ø—Ä–æ–≥—Ä–µ–≤ —á–µ—Ä–µ–∑ predict_on_batch (–∏–ª–∏ —á–µ—Ä–µ–∑ –≤–∞—à tf.function infer_fn)
_ = models['mc_keras'].predict_on_batch(mc_dummy)

for bin_model in binary_models:
    # –æ–¥–∏–Ω–∞–∫–æ–≤–æ ¬´–ø—Ä–æ–≥—Ä–µ–≤–∞–µ–º¬ª —á–µ—Ä–µ–∑ predict_on_batch
    bin_expected = bin_model.input_shape[-1]
    bin_dummy = pad_to_expected(dummy_tfidf, bin_expected).astype(np.float32)
    _ = bin_model.predict_on_batch(bin_dummy)

# –û–±—ë—Ä—Ç–∫–∏ tf.function –¥–ª—è Keras-–º–æ–¥–µ–ª–µ–π (—á—Ç–æ–±—ã –≤—ã–ø–æ–ª–Ω—è—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –≤ –≥—Ä–∞—Ñ–µ, –±–µ–∑ retrace)
# >>> ADDED: –û–ø—Ä–µ–¥–µ–ª—è–µ–º tf.function –æ–¥–∏–Ω —Ä–∞–∑ –≤–Ω–µ —Ü–∏–∫–ª–∞ –∑–∞–ø—Ä–æ—Å–æ–≤:contentReference[oaicite:6]{index=6}
@tf.function
def keras_binary_batch_predict(model, batch):
    return model(batch, training=False)

@tf.function
def keras_multi_batch_predict(batch):
    return models['mc_keras'](batch, training=False)


# –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞: —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Å—Ç–∞—Ç—å—é –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
def split_to_sentences(text):
    # >>> CHANGED: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–± —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
    # –ú–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π (nltk.sent_tokenize –∏ —Ç.–ø.) –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏.
    import re
    # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Ç–æ—á–∫–∞–º, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º –∑–Ω–∞–∫–∞–º, —Å–æ—Ö—Ä–∞–Ω—è—è —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
    sentences = re.split(r'(?<=[.!?]) +', text.strip())
    return [s for s in sentences if s]  # —É–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏


    
# –§—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è Keras –º–æ–¥–µ–ª–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Å—Ç–æ–≥–æ Tokenizer –∏–ª–∏ —Å–ª–æ–≤–∞—Ä—è)
# (–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ –º–æ–¥–µ–ª–∏ –æ–∂–∏–¥–∞—é—Ç –Ω–∞ –≤—Ö–æ–¥ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã MAX_SEQ_LEN)
def tokenize_and_pad(sentences):
    m = models  # CHANGED: –±–µ—Ä—ë–º TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    
    # CHANGED: –≤–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –≤–µ—Å—å –±–∞—Ç—á –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    vecs = m['vectorizer'].transform(sentences).toarray()
    
    # CHANGED: –ø–æ–¥–≥–æ–Ω—è–µ–º/—É—Å–µ–∫–∞–µ–º –¥–æ —Ç—Ä–µ–±—É–µ–º–æ–π –¥–ª–∏–Ω—ã Keras-–º–æ–¥–µ–ª–∏
    expected = m['mc_keras'].input_shape[-1]
    padded = pad_to_expected(vecs, expected)
    
    return padded

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–ø–∏—Å–∫–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
def classify_sentences(sentences):
    results = []
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –±–∞—Ç—á –¥–ª—è –≤—Å–µ—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å—Ä–∞–∑—É (–¥–ª—è Keras –º–æ–¥–µ–ª–µ–π)
    X = tokenize_and_pad(sentences)
    X = X.astype(np.int32)  # —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ–º —Ç–∏–ø –¥–ª—è Keras (int32 –¥–ª—è embedding)
    num_sent = X.shape[0]

    # >>> CHANGED: –ï–¥–∏–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –±–∞—Ç—á–µ–≤—ã–π –∑–∞–ø—É—Å–∫ –≤—Å–µ—Ö –±–∏–Ω–∞—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π:contentReference[oaicite:7]{index=7}
    binary_preds = [
        np.array(model.predict_on_batch(X.astype(np.float32)))
        for model in binary_models
    ]    

    binary_preds = np.stack(binary_preds, axis=1).squeeze()  
    # –¢–µ–ø–µ—Ä—å binary_preds –∏–º–µ–µ—Ç —Ñ–æ—Ä–º—É (num_sent, num_binary_models) —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏ –ø–æ –∫–∞–∂–¥–æ–º—É –±–∏–Ω–∞—Ä–Ω–æ–º—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—É

    # –í—ã–±–∏—Ä–∞–µ–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, —Å—Ä–∞–±–æ—Ç–∞–ª –ª–∏ –∫–∞–∫–æ–π-—Ç–æ –±–∏–Ω–∞—Ä–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É–≤–µ—Ä–µ–Ω–Ω–æ
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∏—Ç–æ–≥–æ–≤—ã—Ö –º–µ—Ç–æ–∫ None (–¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–π–¥—É—Ç –Ω–∞ –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤—É—é –º–æ–¥–µ–ª—å)
    final_labels = [None] * num_sent

    for i, probs in enumerate(binary_preds):
        max_idx  = int(np.argmax(probs))
        max_prob = float(probs[max_idx])
        if max_prob >= CONF_THRESHOLD:
            final_labels[i] = binary_model_names[max_idx]

    # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–º —Ç—Ä–µ–±—É–µ—Ç—Å—è –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    to_multi_idxs = [idx for idx, label in enumerate(final_labels) if label is None]
    if to_multi_idxs:
        to_multi_sentences = [sentences[idx] for idx in to_multi_idxs]
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è —ç—Ç–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–º–æ–∂–Ω–æ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å X, –Ω–æ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –æ—Ç–¥–µ–ª—å–Ω–æ)
        X_multi = tokenize_and_pad(to_multi_sentences)
        X_multi = X_multi.astype(np.int32)
        batch_size = X_multi.shape[0]

        # Keras –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–±–∞—Ç—á)
        multi_preds_keras = np.array(keras_multi_batch_predict(X_multi))  # —Ñ–æ—Ä–º–∞ (batch_size, num_classes)

        # PyTorch XLNet –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–±–∞—Ç—á)
        encodings = xlnet_tokenizer(to_multi_sentences,
                                     return_tensors='pt',
                                     padding=True,
                                     truncation=True,
                                     max_length=MAX_SEQ_LEN)
        with torch.no_grad():
            outputs = xlnet_model(**encodings)
        logits = outputs.logits  # —Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–∞ (batch_size, num_classes)
        multi_preds_torch = torch.softmax(logits, dim=1).cpu().numpy()

        # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø—Ä–æ–≥–Ω–æ–∑—ã –¥–≤—É—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        combined_preds = (multi_preds_keras + multi_preds_torch) / 2.0
        # –í—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        pred_classes = combined_preds.argmax(axis=1)

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ–∑–∏—Ü–∏–∏ final_labels
        for j, idx in enumerate(to_multi_idxs):
            class_idx = int(pred_classes[j])
            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞ (—Ç–µ—Ö–Ω–∏–∫–∏) –ø–æ –∏–Ω–¥–µ–∫—Å—É
            label_name = class_labels[class_idx] if label_encoder else str(class_idx)
            final_labels[idx] = label_name

    return final_labels

# Flask –º–∞—Ä—à—Ä—É—Ç –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
@app.route('/', methods=['GET'])
def index():
    # For a simple JSON response:
    return jsonify({
        "service": "Propaganda Detector API",
        "version": "1.0",
        "endpoints": {
            "health": "/health",
            "predict": "/predict (POST)"
        }
    }), 200

@app.route('/health', methods=['GET'])
def health():
    return jsonify(status="ok"), 200

@app.route('/predict', methods=['POST'])
@cross_origin(origins='*')
def predict():
    data = request.get_json(silent=True)
    if not data or 'text' not in data:
        return jsonify({"error": "No input text provided"}), 400

    text = data['text']
    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    sentences = split_to_sentences(text)
    if not sentences:
        return jsonify({"results": []})

    # >>> CHANGED: –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –±–∞—Ç—á–∞–º–∏, –∏—Å–ø–æ–ª—å–∑—É—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
    predictions = classify_sentences(sentences)

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π
    results = []
    for sent, label in zip(sentences, predictions):
        results.append({"sentence": sent, "label": label})

    return jsonify({"results": results})

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
if __name__ == '__main__':
    port = int(os.getenv("PORT", 5000))
    logging.info(f"–ó–∞–ø—É—Å–∫ –Ω–∞ –ø–æ—Ä—Ç—É {port}")
    app.run(host='0.0.0.0', port=port)
'''

# server.py
import os
os.environ["CUDA_VISIBLE_DEVICES"] = ""
import logging
import urllib.request

import psutil

from flask import Flask, request, jsonify
from flask_cors import CORS, cross_origin

import spacy
import numpy as np
import joblib

import torch
import torch.nn.functional as F
from transformers import XLNetTokenizer, XLNetForSequenceClassification

import keras
from keras.layers import InputLayer
from keras.models import load_model
import tensorflow as tf
from tensorflow.keras.layers import Input, Lambda, Concatenate
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import mixed_precision
mixed_precision.set_global_policy('mixed_float16')

from flask import send_from_directory

from huggingface_hub import hf_hub_download
keras.config.enable_unsafe_deserialization()
tf.config.threading.set_intra_op_parallelism_threads(1)
tf.config.threading.set_inter_op_parallelism_threads(1)

# PyTorch —Ç–æ–∂–µ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç —á–∏—Å–ª–æ –ø–æ—Ç–æ–∫–æ–≤
torch.set_num_threads(1)

CONF_THRESHOLD = 0.5

# ========== –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ==========
MODEL_DIR = "models"
os.makedirs(MODEL_DIR, exist_ok=True)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
logging.basicConfig(level=logging.INFO)

app = Flask(__name__)
CORS(app)

# ========== –ü–∞—Ç—á –¥–ª—è InputLayer ==========
class CustomInputLayer(InputLayer):
    def __init__(self, **kwargs):
        batch_shape = kwargs.pop('batch_shape', None)
        if batch_shape is not None:
            # –£–±–∏—Ä–∞–µ–º batch_shape –∏ —Å—Ç–∞–≤–∏–º input_shape = (dims...,)
            kwargs['input_shape'] = tuple(batch_shape[1:])
        super().__init__(**kwargs)

MODELS = None

# ========== –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π ==========
def log_mem(msg=""):
    pid = os.getpid()
    mem = psutil.Process(pid).memory_info().rss / (1024**2)
    logging.info(f"{msg} PID={pid} RAM={mem:.1f} MiB")
    
def init_models():
    logging.info("‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π‚Ä¶")

    # 1) –°–∫–∞—á–∏–≤–∞–µ–º –≤—Å–µ `.keras` –∏ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã –∏–∑ HF
    hf_repos = {
        "Appeal_to_Authority_model.keras": "brsvaaa/Appeal_to_Authority_model.keras",
        "Bandwagon_Reductio_ad_hitlerum_model.keras": "brsvaaa/Bandwagon_Reductio_ad_hitlerum_model.keras",
        "Black-and-White_Fallacy_model.keras": "brsvaaa/Black-and-White_Fallacy_model.keras",
        "Causal_Oversimplification_model.keras": "brsvaaa/Causal_Oversimplification_model.keras",
        "Slogans_model.keras": "brsvaaa/Slogans_model.keras",
        "Thought-terminating_Cliches_model.keras": "brsvaaa/Thought-terminating_Cliches_model.keras",
        "text_classification_model.keras": "brsvaaa/text_classification_model.keras",
        "vectorizer.joblib": "brsvaaa/vectorizer.joblib",
        "label_encoder.joblib": "brsvaaa/label_encoder.joblib"
    }
    local = { fname: hf_hub_download(repo_id=repo, filename=fname, cache_dir=MODEL_DIR) 
              for fname, repo in hf_repos.items() }

    models = {}
    models['tfidf'] = joblib.load(local["vectorizer.joblib"])
    models['le']    = joblib.load(local["label_encoder.joblib"])
    models['mc_keras'] = load_model(
        local["text_classification_model.keras"],
        custom_objects={'Functional': keras.models.Model, 'InputLayer': CustomInputLayer},
        compile=False
    )
    models['xlnet_tok'] = XLNetTokenizer.from_pretrained("xlnet-base-cased")
    models['xlnet_mc']  = XLNetForSequenceClassification.from_pretrained("brsvaaa/xlnet_trained_model")
    models['xlnet_mc'].eval()
    nlp = spacy.blank("en"); nlp.add_pipe("sentencizer")
    models['nlp'] = nlp

    # –≤–æ—Ç –ø—É—Ç—å, –∫—É–¥–∞ –±—É–¥–µ–º —Å–æ—Ö—Ä–∞–Ω—è—Ç—å/–æ—Ç–∫—É–¥–∞ –∑–∞–≥—Ä—É–∂–∞—Ç—å –≥–æ—Ç–æ–≤—ã–π multi_binary:
    multi_path = os.path.join(MODEL_DIR, "multi_binary.keras")
    if os.path.exists(multi_path):
        logging.info("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–æ—Ç–æ–≤—ã–π multi_binary –º–æ–¥–µ–ª—å‚Ä¶")
        models['multi_binary'] = load_model(
            multi_path,
            custom_objects={'Functional': keras.models.Model, 'InputLayer': CustomInputLayer},
            compile=False
        )
    else:
        logging.info("üîß –°–æ–±–∏—Ä–∞–µ–º multi_binary –∏–∑ 7 –±–∏–Ω–∞—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π‚Ä¶")
        # 1) –ó–∞–≥—Ä—É–∂–∞–µ–º 7 –±–∏–Ω–∞—Ä–Ω—ã—Ö .keras
        submodels = []
        for fname in [
            "Appeal_to_Authority_model.keras",
            "Bandwagon_Reductio_ad_hitlerum_model.keras",
            "Black-and-White_Fallacy_model.keras",
            "Causal_Oversimplification_model.keras",
            "Slogans_model.keras",
            "Thought-terminating_Cliches_model.keras",
        ]:
            m = load_model(
                local[fname],
                custom_objects={'Functional': keras.models.Model, 'InputLayer': CustomInputLayer},
                compile=False
            )
            m.trainable = False
            submodels.append(m)

        # 2) –°—Ç—Ä–æ–∏–º –æ–±—â–∏–π –≥—Ä–∞—Ñ multi_binary
        D0  = models['tfidf'].transform([""]).shape[1]
        inp = Input(shape=(D0,), dtype=tf.float32, name="tfidf_input")
        probs = []
        for m in submodels:
            D_bin = m.input_shape[-1]
            x_bin = Lambda(lambda x, d=D_bin: x[:, :d], name=f"{m.name}_slice")(inp)
            out   = m(x_bin, training=False)
            # –µ—Å–ª–∏ —É –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è 2 –Ω–µ–π—Ä–æ–Ω–∞ ‚Äî –±–µ—Ä—ë–º x[:,1], –∏–Ω–∞—á–µ x[:,0]
            if out.shape[-1] == 2:
                p1 = Lambda(lambda x: tf.expand_dims(x[:,1], axis=-1),
                            name=f"{m.name}_p1")(out)
            else:
                p1 = Lambda(lambda x: tf.expand_dims(x[:,0], axis=-1),
                            name=f"{m.name}_p1")(out)
            probs.append(p1)

        multi_binary = Concatenate(axis=1, name="binary_probs")(probs)
        models['multi_binary'] = Model(inputs=inp, outputs=multi_binary, name="multi_binary")

        # 3) –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏ —á–∏—Å—Ç–∏–º –ø–∞–º—è—Ç—å
        models['multi_binary'].save(multi_path)
        logging.info(f"‚úÖ multi_binary —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {multi_path}")
        del submodels
        tf.keras.backend.clear_session()

        # 4) –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        models['multi_binary'] = load_model(
            multi_path,
            custom_objects={'Functional': keras.models.Model, 'InputLayer': CustomInputLayer},
            compile=False
        )
        logging.info("‚úÖ multi_binary –ø–æ–¥–≥—Ä—É–∂–µ–Ω –∏ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ.")

    return models



def get_models():
    global MODELS
    if MODELS is None:
        MODELS = init_models()
    return MODELS

# ========== –£—Ç–∏–ª–∏—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è ==========
def split_sentences(text: str):
    return [s.text.strip() for s in get_models()['nlp'](text).sents]

def pad_to_expected(x: np.ndarray, target_dim: int):
    cur = x.shape[1]
    if cur < target_dim:
        return np.hstack([x, np.zeros((x.shape[0], target_dim-cur))])
    return x[:, :target_dim]

def predict_binary_batch(sentences):
    m = get_models()
    raw = m['tfidf'].transform(sentences).toarray()
    D0  = raw.shape[1]
    # –ø–æ–¥–≥–æ–Ω—è–µ–º –¥–æ D0 –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –æ–¥–Ω–æ–π –º–æ–¥–µ–ª—å—é
    X = raw.astype(np.float32)
    probs = m['multi_binary'].predict_on_batch(X)    # shape (N,7)
    labels = [None]*len(sentences)
    for i,row in enumerate(probs):
        j = int(np.argmax(row))
        if row[j] > CONF_THRESHOLD:
            labels[i] = BINARY_MODELS[j][1]
    return labels

def predict_xlnet_batch(sentences):
    m = get_models()
    batch = m['xlnet_tok'](
        sentences, return_tensors="pt", truncation=True,
        padding=True, max_length=512
    )
    batch = {k:v.to(m['xlnet_mc'].device) for k,v in batch.items()}
    if next(m['xlnet_mc'].parameters()).dtype == torch.float16:
        for k in batch:
            batch[k] = batch[k].half()
    with torch.no_grad():
        logits = m['xlnet_mc'](**batch).logits
    return F.softmax(logits, dim=1).cpu().numpy()

def predict_keras_batch(sentences):
    m = get_models()
    raw = m['tfidf'].transform(sentences).toarray()
    D1  = m['mc_keras'].input_shape[-1]
    X   = pad_to_expected(raw, D1).astype(np.float32)
    return m['mc_keras'].predict_on_batch(X)

def ensemble_multiclass_predict(text: str):
    # —Å–Ω–∞—á–∞–ª–∞ –±–∏–Ω–∞—Ä–Ω–∞—è –º–æ–¥–µ–ª—å
    bin_labels = predict_binary_batch(text)  # –º–æ–∂–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –∫ batch, —Ç—É—Ç –Ω–∞ —Å—Ç—Ä–æ–∫–µ
    if bin_labels is not None:
        return bin_labels, None
    # –∏–Ω–∞—á–µ –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤—ã–µ
    p1 = predict_xlnet_batch(text)
    p2 = predict_keras_batch(text)
    avg = (p1 + p2)/2
    cls = int(np.argmax(avg, axis=1)[0])
    return cls, avg
    
# ========== Flask-—ç–Ω–¥–ø–æ–∏–Ω—Ç—ã ==========
@app.route('/download/multi_binary', methods=['GET'])
def download_multi_binary():
    return send_from_directory('models', 'multi_binary.keras', as_attachment=True)
    
@app.route('/', methods=['GET'])
def index():
    # For a simple JSON response:
    return jsonify({
        "service": "Propaganda Detector API",
        "version": "1.0",
        "endpoints": {
            "health": "/health",
            "predict": "/predict (POST)"
        }
    }), 200

@app.route('/health', methods=['GET'])
def health():
    return jsonify(status="ok"), 200

@app.route('/predict', methods=['POST','OPTIONS'])
@cross_origin(origins='*')
def predict():
    if request.method=='OPTIONS':
        return jsonify(message='OK'), 200

    data = request.get_json(silent=True) or {}
    text = data.get("text","").strip()
    if not text:
        return jsonify(error="No text provided"), 400

    # 1) –†–∞–∑–±–∏–≤–∞–µ–º –∏ –¥–µ–ª–∞–µ–º –æ–±—â–∏–π –±–∞—Ç—á
    sentences = split_sentences(text)
    N = len(sentences)
    results = []
    chunk_size = 50

    # 1) —Å–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –±–∏–Ω–∞—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ –æ–¥–Ω–∏–º –±–∞—Ç—á–µ–º
    for start in range(0, len(sentences), chunk_size):
        chunk = sentences[start:start+chunk_size]

        # 1) –±–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏ –¥–ª—è —ç—Ç–æ–≥–æ —á–∞–Ω–∫–∞
        bin_labels = predict_binary_batch(chunk)  # [None|label] * len(chunk)

        # 2) –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å —Ç–æ–ª—å–∫–æ —Ç–∞–º, –≥–¥–µ bin_labels == None
        to_multi_idx = [i for i, lbl in enumerate(bin_labels) if lbl is None]
        mc_labels = [None] * len(chunk)
        if to_multi_idx:
            subsent = [chunk[i] for i in to_multi_idx]
            xl = predict_xlnet_batch(subsent)     # (M,C)
            kr = predict_keras_batch(subsent)     # (M,C)
            avg = (xl + kr) / 2.0
            preds = np.argmax(avg, axis=1)
            for idx, cl in zip(to_multi_idx, preds):
                mc_labels[idx] = int(cl)

        # 3) —Å–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —ç—Ç–æ–≥–æ —á–∞–Ω–∫–∞
        for sent, b_lbl, m_lbl in zip(chunk, bin_labels, mc_labels):
            label = b_lbl if b_lbl is not None else m_lbl
            results.append({"sentence": sent, "Multiclass_Prediction": label})

    return jsonify(results=results), 200

if __name__ == '__main__':
    port = int(os.getenv("PORT", 5000))
    logging.info(f"–ó–∞–ø—É—Å–∫ –Ω–∞ –ø–æ—Ä—Ç—É {port}")
    app.run(host='0.0.0.0', port=port)
